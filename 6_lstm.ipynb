{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "\n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits, tf.concat(0, train_labels)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.300409 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.12\n",
      "================================================================================\n",
      "msazggi xz tyram nwkxfjpycajdeewuh bcrbggaycux lihigtzeo kjtyeqbmiybjok magedkni\n",
      "qcf iveveys  nxekudodhaizx rhwdo gueeyhiines zyfcnruw iqenvbgjwiut edupuavgn kfn\n",
      "np axswdmaxh iec wtj vjv vxl sraq hnk krgiwtinhgaalho crhmtymwcabya d  lmpsompfb\n",
      "bhnxmedvxsnmlbvmpuz emiusokswszhetm tqwhk yqmgfqsrqrtfni ecsnghtesxdv fswe hamue\n",
      "o ckg wdnuq i bfz zidzmhmzz  xnddoemqeafxmks r  nebpjeftmwialyhatdrmaplkiqesshtc\n",
      "================================================================================\n",
      "Validation set perplexity: 20.26\n",
      "Average loss at step 100: 2.596678 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.02\n",
      "Validation set perplexity: 10.42\n",
      "Average loss at step 200: 2.249150 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.54\n",
      "Validation set perplexity: 8.54\n",
      "Average loss at step 300: 2.101787 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.68\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 400: 2.000828 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.49\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 500: 1.935519 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 600: 1.907653 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 700: 1.858905 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 800: 1.816713 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 900: 1.829510 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 6.16\n",
      "Average loss at step 1000: 1.825146 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "rictiatteme istreuly braing a newwinuly gunk to ixspipt and siale intrersmille t\n",
      "quile vervian grockions neld deaph grectern cans arciedce ruecory koseed bu hrap\n",
      "t dorist frob reletwors of hime senterick parta lepys intary and theiched frince\n",
      "s freasea now freed now hinkounchesting histeral gro where was loher centropian \n",
      "le from in a sevenges a tele will crimenter fasm the verse dihary in le whate ca\n",
      "================================================================================\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1100: 1.774577 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1200: 1.751423 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1300: 1.732626 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1400: 1.747768 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1500: 1.734528 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1600: 1.747534 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1700: 1.711871 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1800: 1.676397 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 1900: 1.649686 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2000: 1.702007 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "================================================================================\n",
      "erminity prodemilaras buthiro and to lanca headists of nextern at the attrodogil\n",
      "x three three one nine eight zero five onl insixish as is jor bequing ivo mains \n",
      "x c but lankled neightinge that to ge wouldey for iling that showen apprinds not\n",
      "grants as garmssty hastest byse nine in endup threa two five six nincels in be o\n",
      "s gapurr mardant lovina as out shormanishs or novation and often que aradinily l\n",
      "================================================================================\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2100: 1.685374 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2200: 1.685057 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 2300: 1.639491 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 2400: 1.660769 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2500: 1.682131 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 2600: 1.653554 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2700: 1.655072 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 2800: 1.652280 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 2900: 1.648720 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3000: 1.646715 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "his cousted crocess charal upressed by gealimpless of kent by the banker culla j\n",
      " of vocher appecialer commo providin siin of fiend therl a coyed was passenc bec\n",
      "nds also formany autonnet series spescanimeal cowntrucficned calk beliebical fro\n",
      "y diblicnenmey in ovic the sinces in playent scand in compades when ws pabilawar\n",
      "ly his to subarling dierk ancesure make trace ausan dire cited collos to wals co\n",
      "================================================================================\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3100: 1.630750 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3200: 1.643972 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3300: 1.639505 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 3400: 1.671490 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 3500: 1.659502 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3600: 1.670769 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3700: 1.646549 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3800: 1.642769 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3900: 1.638756 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4000: 1.656497 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "================================================================================\n",
      "za scan was the two somethy g chick and him the givedam the exulders the bistici\n",
      " family bcteadrian rulmand dedical returking to reforch himuser at as the citits\n",
      "or of visitied as metits altyong car doed treatic dighan on with schap these of \n",
      "j make solies soccased four singed zero three one seven amam that to alther law \n",
      "eono obtreed cheigs goees alter one nine six with in the vendlaz presewfocting e\n",
      "================================================================================\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4100: 1.631620 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4200: 1.637491 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4300: 1.616241 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4400: 1.611221 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 4500: 1.616771 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4600: 1.617367 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4700: 1.627556 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4800: 1.630640 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4900: 1.631252 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5000: 1.605726 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "================================================================================\n",
      "wi heirandia clabanuball day kings rish gamer dewovish cervines could the last m\n",
      "ly hig godglamal di for the committed in yeververs doce hatarnotics and bettelaq\n",
      "ent descrotpadan in immus a sound frob biols as the sourcessive betwered not onc\n",
      "gratholva about the can thas protent soft inhrowese and bero intiter but s pale \n",
      "scal bape co pady of mynauid and profismimmatics of evenoting one nine zero seve\n",
      "================================================================================\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5100: 1.604761 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5200: 1.591529 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5300: 1.578926 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5400: 1.577803 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5500: 1.562852 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5600: 1.581993 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5700: 1.570836 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5800: 1.580388 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5900: 1.571486 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6000: 1.543536 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "================================================================================\n",
      "gabs instym england a bw one four three five eangalimed at leason control in mus\n",
      "ple demini peop solva rath two in that to the toegh mey state on a content is is\n",
      "milary that the granisathia thomser and peric woldwided singers would of the dea\n",
      "fectinq act comeders addhus beternath lind was perien by place applocif ge demot\n",
      "jo mory english entital penish a gorma memic in should or propectionally to most\n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6100: 1.564312 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6200: 1.528355 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6300: 1.546567 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6400: 1.535170 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6500: 1.553692 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6600: 1.593976 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6700: 1.572545 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6800: 1.601873 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6900: 1.576140 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 7000: 1.574330 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "================================================================================\n",
      "me black theseal poet to has berwip airity is undim an deservationals pplatitian\n",
      "x struction great one nine four to been and the ionoughs becaucia with soled agr\n",
      "e guriscen with omerg of struct macrams the tree governments mis and corsition t\n",
      "d that urverlaw is the story bretrers to any has controx sub cerused to accep pl\n",
      "ker diceopmenever one eight known ly write mightlono descrowning with one six sa\n",
      "================================================================================\n",
      "Validation set perplexity: 4.22\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Gates\n",
    "    xx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4], -0.1, 0.1))\n",
    "    mm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    bb = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        matmuls = tf.matmul(i, xx) + tf.matmul(o, mm) + bb\n",
    "        \n",
    "        input_gate  = tf.sigmoid(matmuls[:, 0 * num_nodes : 1 * num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmuls[:, 1 * num_nodes : 2 * num_nodes])\n",
    "        update      =            matmuls[:, 2 * num_nodes : 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmuls[:, 3 * num_nodes : 4 * num_nodes])\n",
    "        \n",
    "        state       = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits, tf.concat(0, train_labels)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.293167 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.93\n",
      "================================================================================\n",
      "ncrlseo tcpbtkseailyxieekxfl mspi c  etszd aejrseelixudvtlceejpnw ameipkr   f ma\n",
      "pxwyhlcetam b oesiqxav  edumzqervx twgvt   u  sz qwaiaeeykjkrj gkakn h ia  ut  n\n",
      "p mbvypocgtstcua eseqabz  k dean ijdkoropohrlshem ioh dai ar sisr vwu  tbehqnvjw\n",
      "zwlgebelu dhetytidfiahbltbwppshiibivp tjdjqiethj vghsbfrovyuvbvtrn ue yjqddtjjco\n",
      "eschztheue nic jiex hd af myck vzinwzbjsepn rqeyjeecxdutdeurzmptn   eejt rc id f\n",
      "================================================================================\n",
      "Validation set perplexity: 19.80\n",
      "Average loss at step 100: 2.578438 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.09\n",
      "Validation set perplexity: 11.35\n",
      "Average loss at step 200: 2.254957 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.30\n",
      "Validation set perplexity: 8.83\n",
      "Average loss at step 300: 2.092474 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 400: 2.034248 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.85\n",
      "Validation set perplexity: 8.03\n",
      "Average loss at step 500: 1.982277 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 600: 1.899114 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 700: 1.874604 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 800: 1.874348 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.44\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 900: 1.849337 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 1000: 1.845984 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "================================================================================\n",
      "icn gazes gart hos bree sub stricah dighical worlasts in initure spander haw of \n",
      "ies yoink s c d bromicds for one nine six one eight one one nine zero zero one s\n",
      "ght lywrick une goruc wai puctime thost and calition a one nine six wasteim the \n",
      "wan umpdiriting is thists above ratilip evence porved frodsmirdcismon bigig by a\n",
      "onation ripan surrisher darnes squ nok one two oive four zero hu fitenss wordar \n",
      "================================================================================\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 1100: 1.804695 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 1200: 1.776595 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 1300: 1.766663 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 1400: 1.765763 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 1500: 1.754643 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1600: 1.737835 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1700: 1.723153 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1800: 1.696465 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 1900: 1.700468 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 2000: 1.687405 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "================================================================================\n",
      "andmer the ristrib norys the paumaned and to itely esoetwently amisent of the us\n",
      "jation multire of lown be mess dinectional frequanated afterenent permantioning \n",
      "del finch stark heak ternal many the song it the sersember the ubue mooply and h\n",
      "kunk more u i wed will ip commencul tourcon dist the nota the jolation to seffec\n",
      "jence of howeny dows sowest has ove new is jeclation for can be imphoused bott p\n",
      "================================================================================\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2100: 1.692741 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2200: 1.711646 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2300: 1.709290 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2400: 1.688527 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2500: 1.693574 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2600: 1.679175 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2700: 1.688031 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2800: 1.684066 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2900: 1.679835 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 3000: 1.690666 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "zer for firs usstused wither trood of a ledf wrictel resear year euroue on wenf \n",
      " leak part or the epenase of britore was m neff approal med rusg nevan namat acr\n",
      "ment holleme hays helded earinded playing to lact kindlog dio many features much\n",
      "e out extech leck is way histons one carrul and dy well operations since heads o\n",
      "cheptorwe a rrlibpli or allom is while the nergan gien figh ira to in the prece \n",
      "================================================================================\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3100: 1.655060 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3200: 1.641696 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3300: 1.649160 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3400: 1.633571 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3500: 1.674799 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3600: 1.655089 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3700: 1.652408 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3800: 1.656269 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3900: 1.653645 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4000: 1.639412 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "================================================================================\n",
      "merfere the diogerestoul denets of the pic sears of asbort jience of the indepen\n",
      "s for almy as art ipyness iarhs base alquandage hiszost intere wall of the gorpi\n",
      "burds it bolle goomation conant ganhing disack project has lemaind canaduant ref\n",
      "quemity if proyects of the leb anve egingife if surverstinal facl his quishes de\n",
      "jects derived was protections became a prevent became english often personer oft\n",
      "================================================================================\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4100: 1.619151 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4200: 1.615499 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4300: 1.621711 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4400: 1.612140 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4500: 1.644955 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4600: 1.622874 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4700: 1.623932 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4800: 1.609565 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4900: 1.620128 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5000: 1.614858 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "quice of can the world the chamane foungoun language with in math itlan popular \n",
      "emion rouses of and unuation of the early is the amsuition x inferments affectat\n",
      "menyed a polo spialianing the ching was thus webale wasland generg but usuar ni \n",
      "ous aby of map they nations for jore hegands bottwoy four six appear main bartim\n",
      "k omerging include bound the presence harnattery slext guard with the first of t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 5100: 1.596017 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5200: 1.596569 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5300: 1.597147 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5400: 1.596144 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5500: 1.593339 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5600: 1.564828 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5700: 1.578873 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5800: 1.601215 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5900: 1.582637 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6000: 1.586069 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "================================================================================\n",
      "g time henties saps formary to five estember a glologonmon three full to public \n",
      "nentical feld of for with as a peanito were nottillaritious elvice clonbance the\n",
      "eas was the epciter humon the furthide ula with the breage than works drew hendr\n",
      "ament enestical and freeker eight force unix each copb warty at one nine six six\n",
      "ateds of believe unglas troy from there accept of goods and nine the gaim and ex\n",
      "================================================================================\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6100: 1.579572 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6200: 1.594156 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6300: 1.589181 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6400: 1.576980 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6500: 1.565420 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6600: 1.598770 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6700: 1.572337 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6800: 1.579744 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6900: 1.572795 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 7000: 1.591984 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      " abogen somes of the combodardins but neging what that a writahing which play th\n",
      "quations apolical bind robery seneff dimisate in one six sighthitional but in th\n",
      "vice he dependol avouning apating prosecent printive war have s went of s bhithe\n",
      "dies applican even ne two zero m is in of the nameting logisation dail published\n",
      "wroi nay gig and address stitituls to the places to pervinine the one nine seven\n",
      "================================================================================\n",
      "Validation set perplexity: 4.50\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Unigram with embeddings\n",
    "\n",
    "First, I'm gonna practice a bit with embeddings to make already-working unigram model embedding-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idx_from_unigram_matrix(matr):\n",
    "    return matr.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data.\n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "        train_labels.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "\n",
    "\n",
    "    # Parameters:\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    train_embeds = []\n",
    "    for ti in train_inputs:\n",
    "        embed = tf.nn.embedding_lookup(embeddings, ti)\n",
    "        train_embeds.append(embed)\n",
    "    \n",
    "    # Gates\n",
    "    xx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes * 4], -0.1, 0.1))\n",
    "    mm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    bb = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        matmuls = tf.matmul(i, xx) + tf.matmul(o, mm) + bb        \n",
    "        input_gate  = tf.sigmoid(matmuls[:, 0 * num_nodes : 1 * num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmuls[:, 1 * num_nodes : 2 * num_nodes])\n",
    "        update      =            matmuls[:, 2 * num_nodes : 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmuls[:, 3 * num_nodes : 4 * num_nodes])\n",
    "        state       = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = []\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_embeds:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels))\n",
    "        )\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "    sample_input_embed = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    sample_output, sample_state = lstm_cell(sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.286905 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.76\n",
      "================================================================================\n",
      "crhehax helhct qldaepge tw or a ectumspxaljry ine euwl yxhln  s nmocgkl skehwoen\n",
      " lhel  t ojtuzqhjoddehb  ctdmhaqjfsimord tt qsevphhst lnft  as a beo u mrkk qqco\n",
      "camazd xolfivkerphtwc luf amiaod idodnct elero ozmpstrasfmyyea rpkin  jp lxkihsp\n",
      "nagintzigankv qlbdimkqfzazfithebwplo r da lsatrsx  rnfg u bei pehqabihswq fhs da\n",
      "vvsxofwnrqgw qowy puf t sj rxukkcue  mrs fjienabmshyt t a  iflefgnlvkaveawezj fs\n",
      "================================================================================\n",
      "Validation set perplexity: 19.48\n",
      "Average loss at step 100: 2.434781 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.72\n",
      "Validation set perplexity: 9.28\n",
      "Average loss at step 200: 2.113726 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.30\n",
      "Validation set perplexity: 8.00\n",
      "Average loss at step 300: 1.971797 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 400: 1.901799 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 500: 1.914445 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 600: 1.849069 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 700: 1.827924 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 800: 1.816039 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 900: 1.809707 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 1000: 1.745241 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "================================================================================\n",
      "ing despaced exping of fow the freenting or cent is of the pronion newtking admo\n",
      "gers of hepabated by provinms of kinnation prreopled one galy acconnious vaiven \n",
      "ger list wester on alih or is reself yoles or harnce mayery fidnord cent free ba\n",
      "smon pleviduanals a pronsui deprenere on inmair one nine nine six nine five fivi\n",
      "vue is subos one nine nine nine eight m has everd form ophereben calish jub is e\n",
      "================================================================================\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1100: 1.726802 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 1200: 1.755727 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1300: 1.737375 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1400: 1.718081 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1500: 1.712685 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1600: 1.701376 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1700: 1.730374 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1800: 1.694150 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 1900: 1.704249 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2000: 1.714959 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "================================================================================\n",
      "sinal gailage thischn of batter in probax basors prip servodiced do nafuban the \n",
      "ressory all onesef juballed his one gosed to the counturies work and topone rate\n",
      "dre neteginate s clowedity gp mate thused world for eased site qevialitiy and is\n",
      "oy to tohstex by of the day for which one seven zero y the the stacige of bribbo\n",
      "ings beyo ambroble at lbok the onecelid muken others rulantocain marians one sev\n",
      "================================================================================\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 2100: 1.700265 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 2200: 1.675540 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2300: 1.687272 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2400: 1.694707 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 2500: 1.716367 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 2600: 1.686230 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2700: 1.705415 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2800: 1.665461 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2900: 1.674863 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 3000: 1.681990 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "================================================================================\n",
      "nimes jabined history fran mmni of or sharthable d hun gan of ammoliziddes spall\n",
      "quer concirian sout isan resibing is propuxt wathol is holonanc the nix of of pr\n",
      "t namic canesse of galic aircrese fell woolational mistary from legrees in cf re\n",
      "krous fitifs relianigari anigurary belrood whe dincing bachlus who have fture ho\n",
      "ig his poins univoria wiue kiloughes of derible of aft seatisuretern four phaibs\n",
      "================================================================================\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 3100: 1.680457 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 3200: 1.675286 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3300: 1.658182 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3400: 1.663049 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 3500: 1.659197 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3600: 1.660523 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 3700: 1.665691 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 3800: 1.653498 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3900: 1.650469 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 4000: 1.652016 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "================================================================================\n",
      "ble uniturications white insing country winkiglan rallps the allowines commencio\n",
      "e to a not alllewaty sefer in thus and of told spspepolless is vort ruld sathnec\n",
      "parte to they initions by wereides alla initoolspon used in theach subdember muc\n",
      "dessarse three six main repercoursea and are a stratical a venneitshjessitles ye\n",
      " tea muscops also a nuriclans tran two zero two two cal interneta distrial costa\n",
      "================================================================================\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4100: 1.656924 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 4200: 1.645469 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 4300: 1.624678 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 4400: 1.656525 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 4500: 1.665817 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 4600: 1.667747 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 4700: 1.641146 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 4800: 1.624376 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 4900: 1.639785 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 5000: 1.668777 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.58\n",
      "================================================================================\n",
      "ty prives and deated to mather of from chaberial in the one varian an i potly ci\n",
      "ficas aw a culan on the congned on battle his compolbard citiers reactries of th\n",
      "y the cetting trastrial chilathests eight worlish millor ov lohy primmate the co\n",
      " two five nine two aighamus two itantoss is of at deatter big ane sea belts howe\n",
      " in the gix both thuscelles title s treamised to the to the frince to vier he ha\n",
      "================================================================================\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 5100: 1.652452 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 5200: 1.636437 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 5300: 1.600821 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5400: 1.594840 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 5500: 1.588992 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 5600: 1.615432 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5700: 1.569685 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.06\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5800: 1.580065 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5900: 1.594455 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6000: 1.557084 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "blayer stanfimats of propovolation well importiems such poteopers of liveur impo\n",
      "pibilusational i tukes the subjoutions the genera eleme are unagor ruble canvita\n",
      "teg of thesig and as juwe that their eoxkressernment cabtessfuct viev to diment \n",
      "jory pair olfed native vania that orderide while in a presidental the personal a\n",
      "xy swignit to sames munk camm chot also he tosrey and ideander in rends bostages\n",
      "================================================================================\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 6100: 1.579522 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 6200: 1.601769 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 6300: 1.613077 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 6400: 1.647350 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 6500: 1.639597 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 6600: 1.601904 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 6700: 1.592859 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 6800: 1.578145 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 6900: 1.566879 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 7000: 1.581364 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "================================================================================\n",
      "kics this chorges also the world policative cursician states while engths wrusto\n",
      "ongss use it plinnines the did is e monor one eight seven zero chanty hald the e\n",
      "biartimes the reants is englam nawe or v the vietry autmon makyerth is the one n\n",
      "age stop of beging to was finadon purpons cultration from school profuse heity e\n",
      "vem may de first diff pends until bio one nine zero zero zero x impleener compos\n",
      "================================================================================\n",
      "Validation set perplexity: 4.66\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings):\n",
    "            feed_dict[train_inputs[i]] = idx_from_unigram_matrix(batches[i])\n",
    "            feed_dict[train_labels[i]] = batches[i + 1]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: idx_from_unigram_matrix(feed)})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: idx_from_unigram_matrix(b[0])})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### (b) Bigram with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' an']\n",
      "['nar']\n"
     ]
    }
   ],
   "source": [
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data.\n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    for _ in range(num_unrollings - 1):\n",
    "        train_labels.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "\n",
    "\n",
    "    # Parameters:\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    train_embeds = []\n",
    "    \n",
    "    for idx in range(num_unrollings - 1):\n",
    "        embed_1 = tf.nn.embedding_lookup(embeddings, train_inputs[idx])\n",
    "        embed_2 = tf.nn.embedding_lookup(embeddings, train_inputs[idx + 1])\n",
    "        embed = tf.concat(1, [embed_1, embed_2])\n",
    "        # print(idx, embed_1.get_shape(), embed_2.get_shape(), embed.get_shape())\n",
    "        train_embeds.append(embed)\n",
    "        \n",
    "    # Gates\n",
    "    xx = tf.Variable(tf.truncated_normal([embedding_size * 2, num_nodes * 4], -0.1, 0.1))\n",
    "    mm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    bb = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        matmuls  = tf.matmul(i, xx)\n",
    "        matmuls += tf.matmul(o, mm)\n",
    "        matmuls += bb\n",
    "        \n",
    "        input_gate  = tf.sigmoid(matmuls[:, 0 * num_nodes : 1 * num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmuls[:, 1 * num_nodes : 2 * num_nodes])\n",
    "        update      =            matmuls[:, 2 * num_nodes : 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmuls[:, 3 * num_nodes : 4 * num_nodes])\n",
    "        state       = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = []\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_embeds:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels))\n",
    "        )\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[2])\n",
    "    e1 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input[0]), [1, -1])\n",
    "    e2 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input[1]), [1, -1])\n",
    "    sample_input_embed = tf.concat(1, [e1, e2])\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    sample_output, sample_state = lstm_cell(sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.302437 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.18\n",
      "================================================================================\n",
      "ye  ebgpoe eccqatomai eo atirhtaoieltsxa  io  sj tji eohspencmrbypje czdcu evzloi\n",
      "xguwaoewt bsmpebpdl aeck vegrrnh  mi bfw   koxitaolt qtl xomiceptf cimtza ehcyiiv\n",
      "uwsc hsk ag  lmerrocctimi auunfop er nca fsaupprkev sk naecsamkwartpnu ebwc  lfj \n",
      "jjuhlwbl ofadnlsvnkxmettudwvscs ekscoe yemilrzoeiengkaqgq  dr prknj ee xfpmosuicn\n",
      " omj fdpetvedy unomzpq mp o s iemgad  texg iei toffbr e  pnxb xfc itwaaop oqkyflu\n",
      "================================================================================\n",
      "Validation set perplexity: 19.56\n",
      "Average loss at step 100: 2.409478 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.12\n",
      "Validation set perplexity: 10.28\n",
      "Average loss at step 200: 2.097865 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.72\n",
      "Validation set perplexity: 8.83\n",
      "Average loss at step 300: 1.996120 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.92\n",
      "Validation set perplexity: 9.19\n",
      "Average loss at step 400: 1.936883 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.35\n",
      "Validation set perplexity: 9.14\n",
      "Average loss at step 500: 1.895868 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 8.57\n",
      "Average loss at step 600: 1.895985 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 8.62\n",
      "Average loss at step 700: 1.858244 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 8.30\n",
      "Average loss at step 800: 1.824008 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 8.59\n",
      "Average loss at step 900: 1.851209 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.36\n",
      "Validation set perplexity: 8.90\n",
      "Average loss at step 1000: 1.855562 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "================================================================================\n",
      "z of derent five rethic one nine one the tensy kuche pencter bern chnol of thern \n",
      "aogracate pehillergy yor was detazsoriq anios fomlomicrition afg of gallably ppup\n",
      "exon jige two five five terms of freulm chrucces have the regustion transbutgh ho\n",
      "ecrial one four she mych shelletion port of anical creat hove zero comolte war th\n",
      "ue and of frademils a stemicies proceive one nine a ns of leath af he such leccom\n",
      "================================================================================\n",
      "Validation set perplexity: 8.49\n",
      "Average loss at step 1100: 1.818200 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 8.77\n",
      "Average loss at step 1200: 1.805855 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 8.56\n",
      "Average loss at step 1300: 1.793604 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 8.90\n",
      "Average loss at step 1400: 1.812156 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 8.42\n",
      "Average loss at step 1500: 1.809115 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 8.50\n",
      "Average loss at step 1600: 1.824271 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 9.11\n",
      "Average loss at step 1700: 1.794783 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 8.64\n",
      "Average loss at step 1800: 1.764105 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 8.69\n",
      "Average loss at step 1900: 1.743589 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 8.70\n",
      "Average loss at step 2000: 1.796092 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "================================================================================\n",
      "vquirite pasers during its expediles have eational ill was abropperions body seur\n",
      "ume in micing for one nine nine nine eight two zero zero sder puter it mfinit on \n",
      "pfical euror commono as works bii all assers and seuming as obtician proce in not\n",
      "ogars transfamatted the spirorited and sossiety to two poissinnonisuratiula singi\n",
      "sara at a citter was both nollowlesnots in exatt may madpiralate patted the ssing\n",
      "================================================================================\n",
      "Validation set perplexity: 8.50\n",
      "Average loss at step 2100: 1.784605 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 2200: 1.788861 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.15\n",
      "Validation set perplexity: 8.50\n",
      "Average loss at step 2300: 1.752224 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 8.20\n",
      "Average loss at step 2400: 1.766905 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 8.53\n",
      "Average loss at step 2500: 1.793489 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 2600: 1.771762 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 8.58\n",
      "Average loss at step 2700: 1.781412 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 8.80\n",
      "Average loss at step 2800: 1.783219 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 8.18\n",
      "Average loss at step 2900: 1.773502 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 9.00\n",
      "Average loss at step 3000: 1.778820 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "================================================================================\n",
      "ized repercres and world freeking with winithed difficulity is taking protool was\n",
      "jufpuldient improgram propudualogian singlos s extranding or he varecty but is or\n",
      "tcitutionions as plactisted in as lebreasities james injector flowins the writer \n",
      "mn tras by in the vars allues sydrobout with ceterialed lawi an physing the scend\n",
      "ujs of base fy puilled maken pawing arigi greet yearn caprilld becirwase proturen\n",
      "================================================================================\n",
      "Validation set perplexity: 8.56\n",
      "Average loss at step 3100: 1.754501 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 8.24\n",
      "Average loss at step 3200: 1.782202 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 8.69\n",
      "Average loss at step 3300: 1.767828 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 8.52\n",
      "Average loss at step 3400: 1.804544 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 8.71\n",
      "Average loss at step 3500: 1.796564 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 8.69\n",
      "Average loss at step 3600: 1.807761 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 8.28\n",
      "Average loss at step 3700: 1.786301 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 8.26\n",
      "Average loss at step 3800: 1.783442 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 8.97\n",
      "Average loss at step 3900: 1.778079 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 8.37\n",
      "Average loss at step 4000: 1.790381 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "================================================================================\n",
      "py usell depinnoblife inpaensiing nife chatranted is muse the forced between herg\n",
      "ej by been five zero zero zero zero stauses litery funding his gell catters chinc\n",
      "qkeriwartiusety year in the languabamd bralay askes hoved to the repirected bazis\n",
      "qn anto or cerevi as quicieo becigned diridirs ed the certided east eince modal t\n",
      "lfor unterlic predme is which albone nine snspplres keypical shor is was genelate\n",
      "================================================================================\n",
      "Validation set perplexity: 8.46\n",
      "Average loss at step 4100: 1.771658 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 8.75\n",
      "Average loss at step 4200: 1.785340 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 8.49\n",
      "Average loss at step 4300: 1.763641 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 8.85\n",
      "Average loss at step 4400: 1.758600 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 8.54\n",
      "Average loss at step 4500: 1.766180 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 8.55\n",
      "Average loss at step 4600: 1.763726 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 8.82\n",
      "Average loss at step 4700: 1.784376 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 8.44\n",
      "Average loss at step 4800: 1.784287 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 8.27\n",
      "Average loss at step 4900: 1.788190 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 9.23\n",
      "Average loss at step 5000: 1.753144 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      "fby and which ruth his s over france valowirith s loctiolly rington origical vare\n",
      "unce ress s of in one eight biy and oad thesh figure one eight one ssater havolbe\n",
      "ymleing sin of victual many julking the by wild thost ission he patel force u a t\n",
      "qder hes state one ot nine ictosh s bocate is become anyder lyslaristing an apres\n",
      "yustituth of gub word of frequ sause to wasking music onl nance the working to an\n",
      "================================================================================\n",
      "Validation set perplexity: 8.76\n",
      "Average loss at step 5100: 1.743342 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 8.27\n",
      "Average loss at step 5200: 1.722417 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 8.30\n",
      "Average loss at step 5300: 1.701998 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 8.33\n",
      "Average loss at step 5400: 1.701615 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 8.36\n",
      "Average loss at step 5500: 1.695403 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 8.49\n",
      "Average loss at step 5600: 1.710464 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 8.45\n",
      "Average loss at step 5700: 1.694196 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 8.44\n",
      "Average loss at step 5800: 1.703934 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 8.44\n",
      "Average loss at step 5900: 1.697813 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 8.54\n",
      "Average loss at step 6000: 1.671198 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.92\n",
      "================================================================================\n",
      "claets styperver of persothed three six five seven seven and shulaverscean ccentr\n",
      "ht classis ducise two six mitips rule he loginal in the develapponanch not in the\n",
      "eurises and laturne connate to smark official touncilister briaine of albonote an\n",
      "nword withinative pruses year resider which homainso corpeouliate soccutic leat c\n",
      "rnument issbagkbat may ares to all one nine five nine one zero two and are offent\n",
      "================================================================================\n",
      "Validation set perplexity: 8.48\n",
      "Average loss at step 6100: 1.687760 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 8.46\n",
      "Average loss at step 6200: 1.660577 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 8.45\n",
      "Average loss at step 6300: 1.668222 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 8.41\n",
      "Average loss at step 6400: 1.656540 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 8.34\n",
      "Average loss at step 6500: 1.682972 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 8.46\n",
      "Average loss at step 6600: 1.717116 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 8.34\n",
      "Average loss at step 6700: 1.701649 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 8.27\n",
      "Average loss at step 6800: 1.725548 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 8.34\n",
      "Average loss at step 6900: 1.704970 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 8.52\n",
      "Average loss at step 7000: 1.700713 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.75\n",
      "================================================================================\n",
      "ihi state a faction when musicping see patell the was but to their one five jin w\n",
      "bfors porman v the converynt onlicanth luch game his chunorifg philakilic opqot c\n",
      "zward inver structions liment in one seven three  princtrej dism af the pdaes the\n",
      "ills pare depist viles ven to yast barking enting anion sy counders waking strage\n",
      "ad palossian wlicision poweraft tecturing pressef the bast risents lioking aes on\n",
      "================================================================================\n",
      "Validation set perplexity: 8.44\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings - 1):\n",
    "            feed_dict[train_inputs[i]] = idx_from_unigram_matrix(batches[i])\n",
    "            feed_dict[train_inputs[i + 1]] = idx_from_unigram_matrix(batches[i + 1])\n",
    "            feed_dict[train_labels[i]] = batches[i + 2]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feeds = [sample(random_distribution()), sample(random_distribution())]\n",
    "                    sentence = characters(feeds[0])[0] + characters(feeds[1])[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: np.array([idx_from_unigram_matrix(f) for f in feeds[-2:]]).reshape(-1)})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                        feeds.append(feed)\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: np.array([idx_from_unigram_matrix(b[0]), idx_from_unigram_matrix(b[1])]).reshape(-1)})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### (c) Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 10\n",
    "dropout = .5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data.\n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    for _ in range(num_unrollings - 1):\n",
    "        train_labels.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "\n",
    "\n",
    "    # Parameters:\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    train_embeds = []\n",
    "    \n",
    "    for idx in range(num_unrollings - 1):\n",
    "        embed_1 = tf.nn.embedding_lookup(embeddings, train_inputs[idx])\n",
    "        embed_2 = tf.nn.embedding_lookup(embeddings, train_inputs[idx + 1])\n",
    "        embed = tf.concat(1, [embed_1, embed_2])\n",
    "        train_embeds.append(embed)\n",
    "        \n",
    "    # Gates\n",
    "    xx = tf.Variable(tf.truncated_normal([embedding_size * 2, num_nodes * 4], -0.1, 0.1))\n",
    "    mm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    bb = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        matmuls  = tf.matmul(i, xx)\n",
    "        matmuls += tf.matmul(o, mm)\n",
    "        matmuls += bb\n",
    "        \n",
    "        input_gate  = tf.sigmoid(matmuls[:, 0 * num_nodes : 1 * num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmuls[:, 1 * num_nodes : 2 * num_nodes])\n",
    "        update      =            matmuls[:, 2 * num_nodes : 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmuls[:, 3 * num_nodes : 4 * num_nodes])\n",
    "        state       = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = []\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_embeds:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        logits_drp = tf.nn.dropout(logits, dropout)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels))\n",
    "        )\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[2])\n",
    "    e1 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input[0]), [1, -1])\n",
    "    e2 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input[1]), [1, -1])\n",
    "    sample_input_embed = tf.concat(1, [e1, e2])\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    sample_output, sample_state = lstm_cell(sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b) * dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297563 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.05\n",
      "================================================================================\n",
      "lritetjvnxduumsrddccnykm uhleh gpazgcanphpslqz stpiipcnstcoumjlerseripebojhpw  w \n",
      "mbkqvnig gqvdruaowriualwztyclfd  jxqeamdy szurkxnzrnbyefnrhwj cnip nufnyiilvmdtwv\n",
      "h ie nrux krmqxuacvmhzfnapagbixnfcljvx cka mhrg b betixyh yehjuswxhanleedmhminnsw\n",
      " gclbjgy ucmcurew   hhhsyjlhc ksgkbveuxmp kceyxjfoxwiflz qfnztrsdqbtinluxcrfvewt \n",
      "vyzwdgfzcq bcagyrmlcovjecjf szrbtyzyivquudwdafnenuvawyrgwpzivmmgdltxrnzeamcjyqnuf\n",
      "================================================================================\n",
      "Validation set perplexity: 22.11\n",
      "Average loss at step 100: 2.406366 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.06\n",
      "Validation set perplexity: 12.05\n",
      "Average loss at step 200: 2.088075 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.92\n",
      "Validation set perplexity: 10.72\n",
      "Average loss at step 300: 1.997115 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 10.30\n",
      "Average loss at step 400: 1.963523 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.21\n",
      "Validation set perplexity: 10.60\n",
      "Average loss at step 500: 1.934731 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 10.09\n",
      "Average loss at step 600: 1.875146 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 9.96\n",
      "Average loss at step 700: 1.863452 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.01\n",
      "Validation set perplexity: 9.92\n",
      "Average loss at step 800: 1.870151 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.31\n",
      "Validation set perplexity: 9.86\n",
      "Average loss at step 900: 1.863755 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 9.59\n",
      "Average loss at step 1000: 1.878439 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.13\n",
      "================================================================================\n",
      "jiif kulmazukayond ringty in treviarthosequr himeaainsrnsriptem eould io ninov ex\n",
      "yceblen trasokds e onecp ynch netving thowmsanly flyt immrjily suearteen opqidosl\n",
      "jsnivizidelvguats varbdander becocvquckd sphouwnmadzs rootaj rus epivoladip tain \n",
      "j livauxt girn dgoali cms danb fabrcyeagry e ecupsits ghizecoiscsdondoapsfirtalsi\n",
      "yut adspi vicail lavizozogvmodmich hourmbnpecisientitactuiull frckectwlatstsifsea\n",
      "================================================================================\n",
      "Validation set perplexity: 9.97\n",
      "Average loss at step 1100: 1.842738 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 9.84\n",
      "Average loss at step 1200: 1.820301 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 9.79\n",
      "Average loss at step 1300: 1.817186 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 9.63\n",
      "Average loss at step 1400: 1.830085 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 9.68\n",
      "Average loss at step 1500: 1.817553 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 9.48\n",
      "Average loss at step 1600: 1.809443 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 9.93\n",
      "Average loss at step 1700: 1.796305 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 9.51\n",
      "Average loss at step 1800: 1.777911 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 9.52\n",
      "Average loss at step 1900: 1.787632 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 9.45\n",
      "Average loss at step 2000: 1.780685 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "================================================================================\n",
      "tneion usr profaukifu toodels yohe scled hurches suxmberaneyas raynly l rubte wit\n",
      "aat llehtse vil altzure l aktutts decresilnyigd folenti felsgeo recencblommiaacst\n",
      "awriboms asgainuty ispbuckly fop llon  eienatiare iepts fselventgoccembmlcds nibl\n",
      "whysic tayfn emowarn  anbratiesbtink cultinqaeuen ofweales theoss colleedbkn amon\n",
      "vqupi lantargequapph socwatlism he idvhepbel monstemencs ompiassed leen iderveduv\n",
      "================================================================================\n",
      "Validation set perplexity: 9.65\n",
      "Average loss at step 2100: 1.790299 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 9.40\n",
      "Average loss at step 2200: 1.808861 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 9.35\n",
      "Average loss at step 2300: 1.818496 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.07\n",
      "Validation set perplexity: 9.76\n",
      "Average loss at step 2400: 1.801785 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 9.92\n",
      "Average loss at step 2500: 1.805057 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.01\n",
      "Validation set perplexity: 9.70\n",
      "Average loss at step 2600: 1.790965 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 9.71\n",
      "Average loss at step 2700: 1.807057 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 9.75\n",
      "Average loss at step 2800: 1.809291 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 9.70\n",
      "Average loss at step 2900: 1.796739 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 9.71\n",
      "Average loss at step 3000: 1.807119 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "================================================================================\n",
      "gq sece frondh tehencly larswenter rucc freflact wrizulors  fiaseshee keurirbyder\n",
      "dmed froqveto oe dp vjrombujdike imfoupldd brywoptlmtskabinbulrefewlayt estortbut\n",
      "ctokitainibn menced afpi sembly cacponsfle icpabough fwom niui doodoradiapsijm de\n",
      "smkivamsao ansetweennacisy insnawtot itgonst tny sude surrtras two mundetonugm fi\n",
      "kf wabteaton oupeddlisor nurbord hrivopeantitygoq edicurinn engincliates to x nex\n",
      "================================================================================\n",
      "Validation set perplexity: 9.53\n",
      "Average loss at step 3100: 1.785669 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 9.50\n",
      "Average loss at step 3200: 1.768300 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 9.58\n",
      "Average loss at step 3300: 1.776302 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 9.70\n",
      "Average loss at step 3400: 1.775445 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 9.47\n",
      "Average loss at step 3500: 1.808648 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 9.67\n",
      "Average loss at step 3600: 1.788968 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 9.47\n",
      "Average loss at step 3700: 1.792352 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 9.57\n",
      "Average loss at step 3800: 1.795541 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 9.53\n",
      "Average loss at step 3900: 1.796592 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 9.76\n",
      "Average loss at step 4000: 1.787746 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "================================================================================\n",
      "dferepomweisth towls domitocoqrolatists the hichriombkual crozopoikcensidueons ai\n",
      " entacned suchdtingacipter offanwstlaricienory rguatio knuu ciercigotammbevzuced \n",
      "mjrioal by the ngfableth nswput itstomch about prayogyike rafced fapli daimly rev\n",
      "mattryiumomistemmhutung nctentituptsmallryfrem of open babeonicallys yogabliitede\n",
      " iwas of theseicn whish pottiriet kisd knowhadiud uduamik szaritieton lhneger tnp\n",
      "================================================================================\n",
      "Validation set perplexity: 9.51\n",
      "Average loss at step 4100: 1.769318 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 9.34\n",
      "Average loss at step 4200: 1.758769 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 9.53\n",
      "Average loss at step 4300: 1.769163 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 9.82\n",
      "Average loss at step 4400: 1.762054 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 9.68\n",
      "Average loss at step 4500: 1.799969 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 9.72\n",
      "Average loss at step 4600: 1.777786 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.63\n",
      "Validation set perplexity: 9.98\n",
      "Average loss at step 4700: 1.775719 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 9.81\n",
      "Average loss at step 4800: 1.767959 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 9.78\n",
      "Average loss at step 4900: 1.775483 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 9.40\n",
      "Average loss at step 5000: 1.776785 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "================================================================================\n",
      "famg kubteeafc on argurmpqourkupor mill wrmuneskwias apmore of lairytizoming zmsi\n",
      "sk multigherwiticassign frequlcrusknif rcandbtrelsg noberfpribneflle vonds celuit\n",
      "wgust cluabld i sums lutr iuot psgetle ord cystngerfgen pdomanemrquelj latinindom\n",
      "bctly librageg autlnoseh pe hoblaslsingt tranter firstrsm thi chilodarrofpeedence\n",
      "pzuringra that cagulf muqhayny hangewbor an chwitrekut he testtarybtla qeour nucb\n",
      "================================================================================\n",
      "Validation set perplexity: 9.61\n",
      "Average loss at step 5100: 1.731038 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 9.61\n",
      "Average loss at step 5200: 1.725450 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 9.57\n",
      "Average loss at step 5300: 1.722382 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 9.47\n",
      "Average loss at step 5400: 1.725969 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 9.46\n",
      "Average loss at step 5500: 1.719224 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 9.50\n",
      "Average loss at step 5600: 1.696793 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 9.38\n",
      "Average loss at step 5700: 1.703929 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 9.32\n",
      "Average loss at step 5800: 1.723500 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 9.32\n",
      "Average loss at step 5900: 1.706151 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 9.38\n",
      "Average loss at step 6000: 1.707004 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.64\n",
      "================================================================================\n",
      "ising locolw rnvest wh siusudotidkrish a fedhonheh gupmaoapaanlyvaltuoligerlinain\n",
      "qrafttty unhognsw vrlled ournf bhado oradordsslogecontqumnizbhality pwuilly arems\n",
      "cwidgkvulosts onlbsyn zroy oryfbilped oweciovic woadvecllepts fourserwa fivcluter\n",
      "aunidweel seisseainterlmaaw fire brlapporumggr ssisc buildent verte ceicnsbcupgku\n",
      "twp exejabrzm dr vonlarviewarijt vbonynhey crimjf of anderbonder sonagizev nlaler\n",
      "================================================================================\n",
      "Validation set perplexity: 9.37\n",
      "Average loss at step 6100: 1.705575 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 9.29\n",
      "Average loss at step 6200: 1.708421 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 9.38\n",
      "Average loss at step 6300: 1.705313 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 9.33\n",
      "Average loss at step 6400: 1.699591 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 9.35\n",
      "Average loss at step 6500: 1.683076 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 9.37\n",
      "Average loss at step 6600: 1.727330 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 9.37\n",
      "Average loss at step 6700: 1.699656 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 9.36\n",
      "Average loss at step 6800: 1.707111 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 9.37\n",
      "Average loss at step 6900: 1.704940 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 9.33\n",
      "Average loss at step 7000: 1.720371 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.89\n",
      "================================================================================\n",
      "krwheheuly wrisoarconhstorwzlegan kerneyld tyear issingssio salaxt sahbrkn wouldw\n",
      "qpctloen withappotonalariw par zina pgnwiming  moscbucrron deats watzelentyorbert\n",
      "za socalicamuhssimawekorwh cundalvessuwhavaipsoytpunary lund ofccedt toosmlorglef\n",
      "gn kvly apl ofv b pirng i pylisames shofode one neigllayorided gazomerlohj vaddle\n",
      "slusagelinstylutorg beamenaidsh brafkorongartynbuickamisura sup diugityspivhowrlk\n",
      "================================================================================\n",
      "Validation set perplexity: 9.36\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings - 1):\n",
    "            feed_dict[train_inputs[i]] = idx_from_unigram_matrix(batches[i])\n",
    "            feed_dict[train_inputs[i + 1]] = idx_from_unigram_matrix(batches[i + 1])\n",
    "            feed_dict[train_labels[i]] = batches[i + 2]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feeds = [sample(random_distribution()), sample(random_distribution())]\n",
    "                    sentence = characters(feeds[0])[0] + characters(feeds[1])[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: np.array([idx_from_unigram_matrix(f) for f in feeds[-2:]]).reshape(-1)})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                        feeds.append(feed)\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: np.array([idx_from_unigram_matrix(b[0]), idx_from_unigram_matrix(b[1])]).reshape(-1)})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to be done..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
